model_list:
  # Anthropic Claude Models - Latest (December 2025)
  # NOTE: Prompt caching is handled server-side in chat/+server.ts via layered caching
  # (getSystemPromptLayers + addCacheBreakpoints + enforceCacheControlLimit)
  # DO NOT add cache_control_injection_points here - it causes double-caching bugs
  - model_name: claude-opus-4-5
    litellm_params:
      model: anthropic/claude-opus-4-5-20251101
      api_key: os.environ/ANTHROPIC_API_KEY
      max_tokens: 8192

  - model_name: claude-sonnet-4-5
    litellm_params:
      model: anthropic/claude-sonnet-4-5-20250929
      api_key: os.environ/ANTHROPIC_API_KEY
      max_tokens: 8192

  # Alias for legacy model name (dot instead of dash)
  - model_name: claude-sonnet-4.5
    litellm_params:
      model: anthropic/claude-sonnet-4-5-20250929
      api_key: os.environ/ANTHROPIC_API_KEY
      max_tokens: 8192

  # Claude Sonnet 4 (previous generation)
  - model_name: claude-sonnet-4
    litellm_params:
      model: anthropic/claude-sonnet-4-20250514
      api_key: os.environ/ANTHROPIC_API_KEY
      max_tokens: 8192

  # Claude 3.7 Sonnet (replaces 3.5 Sonnet)
  - model_name: claude-3-7-sonnet
    litellm_params:
      model: anthropic/claude-3-7-sonnet-20250219
      api_key: os.environ/ANTHROPIC_API_KEY
      max_tokens: 8192

  - model_name: claude-3-5-haiku
    litellm_params:
      model: anthropic/claude-3-5-haiku-20241022
      api_key: os.environ/ANTHROPIC_API_KEY
      max_tokens: 8192

  # Claude Haiku 4.5 (Fast & cost-effective)
  - model_name: claude-haiku-4-5
    litellm_params:
      model: anthropic/claude-haiku-4-5-20251001
      api_key: os.environ/ANTHROPIC_API_KEY
      max_tokens: 8192

  # OpenAI Models - Latest (December 2025)

  # GPT-5.2 Series (Latest flagship - 400K context)
  - model_name: gpt-5.2
    litellm_params:
      model: openai/gpt-5.2
      api_key: os.environ/OPENAI_API_KEY
      max_tokens: 128000

  - model_name: gpt-5.2-pro
    litellm_params:
      model: openai/gpt-5.2-pro
      api_key: os.environ/OPENAI_API_KEY
      max_tokens: 128000

  - model_name: gpt-5.2-chat-latest
    litellm_params:
      model: openai/gpt-5.2-chat-latest
      api_key: os.environ/OPENAI_API_KEY
      max_tokens: 128000

  # GPT-5.1 Series (Flagship for coding & agentic tasks)
  - model_name: gpt-5.1
    litellm_params:
      model: openai/gpt-5.1
      api_key: os.environ/OPENAI_API_KEY
      max_tokens: 128000

  - model_name: gpt-5.1-chat-latest
    litellm_params:
      model: openai/gpt-5.1-chat-latest
      api_key: os.environ/OPENAI_API_KEY
      max_tokens: 128000

  # GPT-5.2 Codex (Latest agentic coding model - uses Responses API)
  - model_name: gpt-5.2-codex
    litellm_params:
      model: openai/gpt-5.2-codex
      api_key: os.environ/OPENAI_API_KEY
      max_tokens: 128000

  # GPT-5.1 Codex Models (Specialized for agentic coding)
  - model_name: gpt-5.1-codex-max
    litellm_params:
      model: openai/gpt-5.1-codex-max
      api_key: os.environ/OPENAI_API_KEY
      max_tokens: 128000

  - model_name: gpt-5.1-codex-mini
    litellm_params:
      model: openai/gpt-5.1-codex-mini
      api_key: os.environ/OPENAI_API_KEY
      max_tokens: 64000

  # GPT-4o Series (Multimodal - text, images, audio)
  - model_name: gpt-4o
    litellm_params:
      model: openai/gpt-4o
      api_key: os.environ/OPENAI_API_KEY
      max_tokens: 16384

  - model_name: gpt-4o-mini
    litellm_params:
      model: openai/gpt-4o-mini
      api_key: os.environ/OPENAI_API_KEY
      max_tokens: 16384

  # GPT-4.1 Series (1M context window)
  - model_name: gpt-4.1
    litellm_params:
      model: openai/gpt-4.1
      api_key: os.environ/OPENAI_API_KEY
      max_tokens: 32768

  - model_name: gpt-4.1-mini
    litellm_params:
      model: openai/gpt-4.1-mini
      api_key: os.environ/OPENAI_API_KEY
      max_tokens: 32768

  # O-Series Reasoning Models
  - model_name: o3
    litellm_params:
      model: openai/o3
      api_key: os.environ/OPENAI_API_KEY

  - model_name: o3-mini
    litellm_params:
      model: openai/o3-mini
      api_key: os.environ/OPENAI_API_KEY

  - model_name: o4-mini
    litellm_params:
      model: openai/o4-mini
      api_key: os.environ/OPENAI_API_KEY

# ============================================
  # AWS BEDROCK MODELS - Via Cross-Region Inference Profiles
  # Note: Use us.* prefix for cross-region inference (required for on-demand)
  # ============================================

  # Meta Llama 3.3 70B - Best open source general purpose
  - model_name: llama-3-3-70b
    litellm_params:
      model: bedrock/us.meta.llama3-3-70b-instruct-v1:0
      aws_access_key_id: os.environ/AWS_ACCESS_KEY_ID
      aws_secret_access_key: os.environ/AWS_SECRET_ACCESS_KEY
      aws_region_name: os.environ/AWS_REGION_NAME
      max_tokens: 4096

  # Meta Llama 3.1 8B - Fast and cost-effective
  - model_name: llama-3-1-8b
    litellm_params:
      model: bedrock/us.meta.llama3-1-8b-instruct-v1:0
      aws_access_key_id: os.environ/AWS_ACCESS_KEY_ID
      aws_secret_access_key: os.environ/AWS_SECRET_ACCESS_KEY
      aws_region_name: os.environ/AWS_REGION_NAME
      max_tokens: 4096

  # Meta Llama 4 Scout 17B - Massive context window
  - model_name: llama-4-scout
    litellm_params:
      model: bedrock/us.meta.llama4-scout-17b-instruct-v1:0
      aws_access_key_id: os.environ/AWS_ACCESS_KEY_ID
      aws_secret_access_key: os.environ/AWS_SECRET_ACCESS_KEY
      aws_region_name: os.environ/AWS_REGION_NAME
      max_tokens: 8192

  # Amazon Nova Pro - Replaces Titan Premier (Titan is EOL Aug 2025)
  - model_name: nova-pro
    litellm_params:
      model: bedrock/amazon.nova-pro-v1:0
      aws_access_key_id: os.environ/AWS_ACCESS_KEY_ID
      aws_secret_access_key: os.environ/AWS_SECRET_ACCESS_KEY
      aws_region_name: os.environ/AWS_REGION_NAME
      max_tokens: 4096

  # DeepSeek R1 - Strong reasoning model
  - model_name: deepseek-r1
    litellm_params:
      model: bedrock/us.deepseek.r1-v1:0
      aws_access_key_id: os.environ/AWS_ACCESS_KEY_ID
      aws_secret_access_key: os.environ/AWS_SECRET_ACCESS_KEY
      aws_region_name: os.environ/AWS_REGION_NAME
      max_tokens: 8192

  # Meta Llama 4 Maverick 17B - 1M context, vision, tool use (no streaming)
  - model_name: llama-4-maverick
    litellm_params:
      model: bedrock/us.meta.llama4-maverick-17b-instruct-v1:0
      aws_access_key_id: os.environ/AWS_ACCESS_KEY_ID
      aws_secret_access_key: os.environ/AWS_SECRET_ACCESS_KEY
      aws_region_name: os.environ/AWS_REGION_NAME
      max_tokens: 8192

  # Mistral Large 3 - 256K context, vision, tools (no streaming with tools)
  - model_name: mistral-large-3
    litellm_params:
      model: bedrock/mistral.mistral-large-3-675b-instruct
      aws_access_key_id: os.environ/AWS_ACCESS_KEY_ID
      aws_secret_access_key: os.environ/AWS_SECRET_ACCESS_KEY
      aws_region_name: os.environ/AWS_REGION_NAME
      max_tokens: 8192

  # DeepSeek V3.1 - 128K context, thinking mode (no Converse API tools)
  - model_name: deepseek-v3
    litellm_params:
      model: bedrock/deepseek.v3-v1:0
      aws_access_key_id: os.environ/AWS_ACCESS_KEY_ID
      aws_secret_access_key: os.environ/AWS_SECRET_ACCESS_KEY
      aws_region_name: us-west-2
      max_tokens: 8192

  # MiniMax M2 - 456B hybrid MoE, 1M context, strong reasoning
  - model_name: minimax-m2
    litellm_params:
      model: bedrock/minimax.minimax-m2
      aws_access_key_id: os.environ/AWS_ACCESS_KEY_ID
      aws_secret_access_key: os.environ/AWS_SECRET_ACCESS_KEY
      aws_region_name: os.environ/AWS_REGION_NAME
      max_tokens: 8192

  # Google Gemma 3 27B - Multimodal open model, 128K context
  - model_name: gemma-3-27b
    litellm_params:
      model: bedrock/google.gemma-3-27b-it
      aws_access_key_id: os.environ/AWS_ACCESS_KEY_ID
      aws_secret_access_key: os.environ/AWS_SECRET_ACCESS_KEY
      aws_region_name: os.environ/AWS_REGION_NAME
      max_tokens: 8192

  # Moonshot Kimi K2 Thinking - 1T MoE (32B active), 256K context, strong reasoning
  - model_name: kimi-k2-thinking
    litellm_params:
      model: bedrock/moonshot.kimi-k2-thinking
      aws_access_key_id: os.environ/AWS_ACCESS_KEY_ID
      aws_secret_access_key: os.environ/AWS_SECRET_ACCESS_KEY
      aws_region_name: os.environ/AWS_REGION_NAME
      max_tokens: 8192

# ============================================
  # GOOGLE GEMINI MODELS - Direct API
  # ============================================

  # Gemini 3 Pro - Latest flagship reasoning model
  - model_name: gemini-3-pro
    litellm_params:
      model: gemini/gemini-3-pro-preview
      api_key: os.environ/GOOGLE_API_KEY
      max_tokens: 65536

  # Gemini 3 Flash - Fast frontier-class performance at low cost
  - model_name: gemini-3-flash
    litellm_params:
      model: gemini/gemini-3-flash-preview
      api_key: os.environ/GOOGLE_API_KEY
      max_tokens: 65536

  # Gemini 2.5 Pro - Best for complex reasoning (code, math, STEM)
  - model_name: gemini-2.5-pro
    litellm_params:
      model: gemini/gemini-2.5-pro
      api_key: os.environ/GOOGLE_API_KEY
      max_tokens: 65536

  # Gemini 2.5 Flash - Best price-performance with thinking
  - model_name: gemini-2.5-flash
    litellm_params:
      model: gemini/gemini-2.5-flash
      api_key: os.environ/GOOGLE_API_KEY
      max_tokens: 65536

litellm_settings:
  drop_params: true
  set_verbose: false
  cache: false
  
router_settings:
  model_group_alias:
    claude: ["claude-opus-4-5", "claude-sonnet-4-5", "claude-sonnet-4", "claude-3-7-sonnet", "claude-haiku-4-5", "claude-3-5-haiku"]
    gpt: ["gpt-5.2", "gpt-5.2-pro", "gpt-5.1", "gpt-4o", "gpt-4o-mini", "gpt-4.1", "gpt-4.1-mini"]
    codex: ["gpt-5.2-codex", "gpt-5.1-codex-max", "gpt-5.1-codex-mini"]
    reasoning: ["o3", "o3-mini", "o4-mini"]
    llama: ["llama-3-3-70b", "llama-3-1-8b", "llama-4-scout", "llama-4-maverick"]
    mistral: ["mistral-large-3"]
    deepseek: ["deepseek-r1", "deepseek-v3"]
    gemini: ["gemini-3-pro", "gemini-3-flash", "gemini-2.5-pro", "gemini-2.5-flash"]
    minimax: ["minimax-m2"]
    gemma: ["gemma-3-27b"]
    moonshot: ["kimi-k2-thinking"]
    bedrock: ["llama-3-3-70b", "llama-3-1-8b", "llama-4-scout", "llama-4-maverick", "nova-pro", "deepseek-r1", "deepseek-v3", "mistral-large-3", "minimax-m2", "gemma-3-27b", "kimi-k2-thinking"]

  fallback_models:
    claude-opus-4-5: ["claude-sonnet-4-5", "claude-sonnet-4"]
    claude-sonnet-4-5: ["claude-sonnet-4", "claude-3-7-sonnet"]
    claude-sonnet-4: ["claude-3-7-sonnet", "claude-3-5-haiku"]
    claude-3-7-sonnet: ["claude-sonnet-4", "claude-haiku-4-5", "claude-3-5-haiku"]
    claude-haiku-4-5: ["claude-3-5-haiku", "claude-3-7-sonnet"]
    gpt-5.2: ["gpt-5.2-pro", "gpt-5.1", "gpt-4o"]
    gpt-5.2-pro: ["gpt-5.2", "gpt-5.1", "gpt-4o"]
    gpt-5.1: ["gpt-5.2", "gpt-4o", "gpt-4.1"]
    gpt-5.2-codex: ["gpt-5.1-codex-max", "gpt-5.1-codex-mini", "gpt-5.2"]
    gpt-5.1-codex-max: ["gpt-5.2-codex", "gpt-5.1-codex-mini", "gpt-5.1"]
    gpt-4o: ["gpt-4o-mini", "gpt-4.1", "gpt-5.1"]
    gpt-4.1: ["gpt-4.1-mini", "gpt-4o"]
    o3: ["o3-mini", "o4-mini"]
    o3-mini: ["o4-mini"]

general_settings:
  # Logging
  log_level: INFO

  # Timeout settings
  request_timeout: 600
  stream_timeout: 60
